%\documentclass[apj]{emulateapj}
%\documentclass[preprint]{emulateapj}  %{aastex}
%\documentclass[manuscript]{aastex}
%\documentclass[preprint2]{emulateapj}
\documentclass[preprint2]{aastex}
\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{deluxetable}
%\usepackage[charter]{mathdesign}
\usepackage{bm}
%\usepackage{breqn}
\usepackage{natbib}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\bfOmega}{\,\mathbf{\Omega}}
\newcommand{\bfv}{\,\mathbf{v}}
\newcommand{\bfu}{\,\mathbf{u}}
\newcommand{\bfr}{\,\mathbf{r}}
\newcommand{\bfk}{\,\mathbf{k}}
\newcommand{\hstp}{\,{\frac{\ud s}{2}}}
\newcommand{\rex}{\, R_\mathrm{ex}}
\newcommand{\rout}{\, R_\mathrm{out}}
\newcommand{\rin}{\, R_\mathrm{in}}
\newcommand{\zsp}{\, Z_\mathrm{sp}}
\newcommand{\vex}{\, V_\mathrm{ex}}
\newcommand{\vin}{\, V_\mathrm{in}}
\newcommand{\circf}{\, \mathrm{circ}}
\newcommand{\xringf}{\, \mathrm{xring}}
\newcommand{\gax}{\, g_\mathrm{ax}}
\newcommand{\bp}{\,\mathbf{p}}
\newcommand{\bu}{\,{\bf u}}
%\newcommand{\bt}{\,{\bm {\vartheta}}}
%\newcommand{\bt}{\,{\mathsf \vartheta}}
%\newcommand{\bt}{\,{\boldsymbol \vartheta}}
\newcommand{\bx}{\,\mathbf{x}}


\begin{document}

\title{Massively Parallel Markov Chain Monte-Carlo Software for Non-Linear Optimization and Equation Solving}

\author{Leonid Benkevitch, Kazunori Akiyama}
\affil{MIT Haystack Observatory, Westford MA}
\email{benkev@haystack.mit.edu}

\begin{abstract} \small 
For model fitting we use a version of the Markov chain Monte-Carlo (MCMC) algorithm based on the Metropolis-Hastings sampler with replica exchange. Over a series of simulations we demonstrate that our model can be used for determining geometric measures of a black hole, thus providing information on the shadow size, linking General Relativity with accretion theory. \\
\end{abstract}

\tableofcontents
\newpage


\section{Introduction and Motivation}

In the problems on finding global extrema of non-linear functions, gradient methods of optimization are powerless before multimodal functions, non-continuous functions, or functions some (or all) of whose arguments can only take discrete or even integer values. Statistical methods can be more helpful for this class of problems. 

The method described in this paper initially was used in a radio astronomy problem of a black hole event horizon imaging. The scarce and sparse data from the VLBI radio array pointed at the Sagittarius A* (Sgr A*) object, the super massive black hole at the center of our Galaxy, did not allow to use traditional imaging tools. However, some existing models could give approximate notion of the black hole look. We decided to create a model of the black hole image composed of simple geometric forms. Varying the geometry and intensities of the forms, the model was supposed to take the approximate image of any of the black hole states. This gave a chance to measure some of the real black hole parameters. Finding the model whose brightness image is the closest to the one of the real black hole required fitting it to the observation data. This was a problem on finding the absolute (or global) minimum of the $\chi^2$ value for the observation data $x_i^{\rm obs}$ versus the model data $x^{\rm mod}(\bp,u_i,v_i)$ (here the visibility amplitudes and closure phases): 
\begin{equation}
  \label{chi2}
  \chi^2(\bp) = \sum_{i=1}^{N} \frac{(x^{\rm mod}(\bp,u_i,v_i) - x_i^{\rm obs})^2}{\sigma_i^2},
\end{equation}
where $f^{\rm mod}()$ was the visibility model,  $\bp=(p_1,p_2,...,p_{N_p})$ was the model parameter vector, $(u_i,v_i)$ were the coordinates of measured visibilities in the spatial frequency space, and the variance of every datum $\sigma_i^2$ played the role of its weight in the $\chi^2$ sum. The model had 9 parameters, so had $\chi^2$, and the problem was in finding the global minimum of the 9-dimensional hypersurface in the 10-dimensional space. This $\chi^2$ hypersurface had extremely complex relief: it had innumerable creases and local minima and the gradient methods appeared useless. 

We created a massively parallel and highly efficient, problem-specific optimization code based on the CUDA computing platform to be run on the Nvidia graphic cards (GPU). The code would take just seconds to fit our model to the observations. Subsequent modifications turned the code to a universal tool capable of fast solving general non-linear optimization problems and even solving systems of non-linear equations. 

We briefly describe here the statistical method we used in our software, the Markov Chain Monte-Carlo (MCMC) with Replica Exchange. It is based on the idea of the Bayesian inference. 



\section{Bayesian Inference}

In the Bayes paradigm, the new information, the ``evidence", is used to update the ``prior" guess on the probability of a hypothesis, with the use of the well known Bayes' theorem
\begin{equation}
  \label{bayes_rule}
  P(A|B) =  \frac{P(B|A)P(A)}{P(B)}.
\end{equation}
From the prior probability of event $A$, $P(A)$, its posterior probability, $P(A|B)$, is inferred, having the evidence $P(B)$ and the likelihood $P(B|A)$. Instead of single events $A$ and $B$ and their (conditional) probabilities the probability distributions defined on generally multidimensional spaces can be considered. The Bayes' theorem (Eqn.~\eqref{bayes_rule}) can be used to calculate the posterior distribution at any point using the values of the three other distributions. In case of the model fitting this is used to obtain the posterior distribution of the fitted model parameters from the distribution of the observation data and the ``prior" model parameter distribution. Numerically it can be implemented as  probing the parameter space evenly enough to get the sufficiently dense set of points to plot the histogram of posterior distribution. Its maximum (or maxima) will be at the best-fit parameter values.

The optimization problem of fitting the radio observations $x_i^{\rm obs}$ to the visibility model $x^{\rm mod}$ by varying the model parameters $\bp$ can be formulated as minimization of Eq.~\eqref{chi2}:
\begin{equation*}
  \label{chi2_nonumber}
  \chi^2(\bp) = \sum_{i=1}^{N} \frac{(x^{\rm mod}(\bp,u_i,v_i) - x_i^{\rm obs})^2}{\sigma_i^2}  \rightarrow {\min\limits_{\bp}},
\end{equation*}

We assume that the specific set of observed data $x_i^{\rm obs}$ presented as a vector $\bx$ (visibility amplitudes and closure phases) is a sample from the multidimensional random variable, ${\bf X}$, with the probability density distribution $P({\bf X})$. For a sample $\bx$, $P(\bx)$ is a single number, the value of $P({\bf X})$ at the point $\bx$. Further, the lower case letters are used instead of the capitals, so $P(\bx)$ actually means $P({\bf X})$.

Within the Bayesian framework both the observed data set $\bf{x}$ and the model parameter vector $\bp$ are considered as statistically linked multi-dimensional random variables with their joint probability distribution
\begin{equation}
  \label{joint_ux}
  P(\bp,\bx) =  P(\bp|\bx)P(\bx) = P(\bx|\bp)P(\bp),
\end{equation}
where ``," reads ``and". Relationship \eqref{joint_ux} associates the probability densities named as follows:
\begin{itemize}
  \item[-] $P(\bx)$ the evidence,
  \item[-] $P(\bp)$ the prior or prior distribution,
  \item[-] $P(\bx|\bp)$ the likelihood, and
  \item[-] $P(\bp|\bx)$ the inference or the posterior probability distribution.
\end{itemize}
In terms of causality, the object under observation is the cause, and the observation data is the effect. The Bayes' theorem allows us to rearrange the cause and the effect: using the known data $\bx$, compute the \emph{posterior} probability distribution $P(\bp|\bx)$ that $\bx$ is an effect of the object represented by our model with the parameter set $\bp$. Thus, we pose a task to find not just a single set of the ``optimal" model parameters, but the probability distribution of this set over the parameter space \emph{given} the actual set of observation data. Of course, we are interested in such distributions for every single parameter, which are the marginal distributions of $P(\bp|\bx)$. Below is shown that MCMC allows direct rendering of these marginal distributions. 

Here the Bayesian distributions have been explained in terms of the visibility model fitting to the observation data. However, nothing hampers us from formulating any optimization problem in these terms. For example, a general problem of finding the minima of a vector function ${\bf x} = {\bf F}(\bp,\bfv)$, where ${\bf F} = [f_1, f_2, ... f_N]^T$ and ${\bf x} = [x_1, x_2, ... x_N]^T$, can be formulated as
\begin{equation}
  \label{vector_optimization_problem}
  \chi^2(\bp) = \sum_{i=1}^{N} \frac{(f_i(\bp,\bfv) - x_i)^2}{\sigma_i^2}  \rightarrow {\min\limits_{\bp}},
\end{equation}
where $\bfv$ are some immutable parameters. Here the ``data" $\bx$ can be just a vector of zeros, $\bx~\equiv~{\bf 0}$. Moreover, the same MCMC procedure can find \emph{all} the roots $\bp_k$ of a non-linear system of simultaneous equations ${\bf F}(\bp,\bfv) = {\bf 0}$, if it is consistent and the size of vector $\bp$ equals the number of equations $N$:
\begin{equation}
  \label{vector_optimization_problem}
  \chi^2(\bp) = \sum_{i=1}^{N} \frac{f_i^2(\bp,\bfv)}{\sigma_i^2}  \rightarrow {\min\limits_{\bp}}.
\end{equation}
The roots will sit at the locations $k$ where $\chi(\bp_k)$ is close to zero.

The problem of minimization of a single function of several variables $x = f(\bp,\bfv)$ is quite trivial:
\begin{equation}
  \label{optimization_problem}
  \chi^2(\bp) =  \frac{f^2(\bp,\bfv)}{\sigma^2} \rightarrow {\min\limits_{\bp}}.
\end{equation}
Here the ``data" $x$ is just a zero. The $\chi^2$ is simply $f^2/\sigma^2$. In problems like the mentioned above, all the variances $\sigma_i$ can be set to the same nonzero value, for example, to unity.

The likelihood $P(\bx|\bp)$ may be any positive function that reaches its maximum when the difference between the actual data and the model data becomes zero. We use a Gaussian likelihood
\begin{equation}
  \label{gaussian_likelihood}
  P(\bx|\bp) = \left(\prod_i \frac{1}{\sqrt{2 \pi} \sigma_i} \right) e^{-\frac{1}{2}\chi^2},
\end{equation}
where $\chi^2$ and $\sigma$ are from Eq.~\eqref{chi2}.
The prior, $P(\bp)$, is the distribution over the parameter space that represents our preliminary knowledge about the intervals where the parameter values could be present.
%, or sometimes algebraic inequalities relating several parameters.
The prior may not be very informative (for example, a uniform value within the allowed interval and zero outside), but it must always be provided.  


Dividing \eqref{joint_ux} by $P(\bx)$ yields the Bayes' formula:
\begin{equation}
  \label{bayes_formula}
  P(\bp|\bx) =  \frac{P(\bx|\bp)P(\bp)}{P(\bx)},
\end{equation}
with the searched for posterior parameter distribution on the left hand side, and computable probabilities on the right hand side. The value of $P(\bx)$, the probability density of the given observation data sample, can be calculated using the total probability law,
\begin{equation}
  P(\bx) = \int P(\bx|\bp)P(\bp) \ud \bp.
\end{equation}
For a given prior and a model the evidence $P(\bx)$ is always a single constant value as long as we work with the same data set: the integration over the whole parameter space removes all the variables. The evidence value can be used to compare the quality of different models. A ``better" model will have larger $P(\bx)$. The Bayes' theorem thus takes the form
\begin{equation}
  \label{post_distr_int}
  P(\bp|\bx) =  \frac{P(\bx|\bp)P(\bp)}{\int P(\bx|\bp)P(\bp) \ud \bp}.
\end{equation}
As we already said, the posterior distribution of all the parameters, $P(\bp|\bx)$, is not as interesting as that of an individual parameter, $P(p_i|\bx)$, $p_i \in \bp$, because it can provide the information on the mean value (or values, if multi-modal) and uncertainty of the estimate of the parameter $p_i$. Such individual distributions for every parameter $p_i$ are, in effect, the marginal distributions, i.e. the results of integration of the total distribution $P(\bp|\bx)$ over the parameter subspace spanned by all the parameters but $p_i$:
\begin{equation}
  \label{param_post_distr}
  P(p_i|\bx) =  \int P(\bp|\bx)\ud p_1 \ud p_2 ... \ud p_{i-1} \ud p_{i+1} ... \ud p_N.
\end{equation}
The posterior distributions, $P(p_i|\bx)$, are not required to be normalized, so the strict equations~\eqref{bayes_formula} or~\eqref{post_distr_int} can be relaxed to a mere proportionality
\begin{equation}
  \label{bayes_propor}
  F(\bp|\bx) \propto P(\bx|\bp)P(\bp),
\end{equation}
where $F(\bp|\bx) \propto P(\bp|\bx)$. Normalization of the $F$ function would produce the posterior distribution $P(\bp|\bx)$ and its marginals $P(p_i|\bx)$. However, statistical parameters of $P(p_i|\bx)$ -- the means and the standard deviations, or qualitative conclusions about their forms -- can be found directly from $F(p_i|\bx)$ without the normalization. The Metropolis-Hastings algorithm described here utilizes this fact. It draws many samples from the $P(p_i|\bx)$ distributions, and the result of optimization, $\bp$, is obtained from the histograms built using the saved samples. 




\section{Markov Chain Monte Carlo as Implementation of Bayesian Inference}

The Bayesian inference is applied to the optimization problems where the target functions are non-smooth multidimensional hypersurfaces, e.g., $\chi^2$ in case of finding the best-fit model parameters. Because of the innumerable local minima, the random exploration of the whole domain and finding the approximations of parameter distributions appears to be preferred to the gradient descent methods. 
We use a strong algorithm named Markov Chain Monte Carlo (MCMC) with Replica Exchange (or Parallel Tempering) based on the improved Metropolis-Hastings algorithm. 


\subsection{Metropolis-Hastings Algorithm}

The algorithm has three stages. First, an initial set of parameters $\bp_0$ is randomly drawn from the prior distribution $P(\bp)$. The two other stages, the burn-in and the search, are essentially the same except at the burn-in stage the optimal steps for each parameter are picked. The iterations generate the Markov chain of the parameter tuples $\bp_i = \left(p_{i,1},\, p_{i,2},\, ...\, p_{i,N_p} \right)$, and the more iterations, the better the $\bp_i$ values approximate $P(\bp|\bx)$. The Markov property, i.e. the dependence of the $i_{\rm th}$ chain element on the previous $(i-1)_{\rm th}$ element only is ensured by the method of their generation. At each iteration, a \emph{proposal} model parameter set $\bp^\prime$ is generated from the proposal distribution $q(\bp_{i-1};\bp^\prime)$. The new proposal set is randomly accepted or rejected with a probability $\alpha$,
\begin{equation}
  \label{accept_probab}
  \alpha =  \min \left( \frac{P(\bx|\bp')P(\bp')q(\bp_{i-1};\bp')} 
            {P(\bx|\bp_{i-1})P(\bp_{i-1})q(\bp';\bp_{i-1})}, 1 \right).
\end{equation}
In the Metropolis-Hastings algorithm the proposal distribution $q(\bp_i;\bp_j)$ generally does not necessarily \emph{have to} be symmetric. However, the question of the best choice of $q$ is still unclear. In order to simplify the calculations, we use a symmetric proposal distribution $q$ as in the original Metropolis algorithm. Here it is  assumed a Gaussian distribution
\begin{equation}
  \label{gaussian_prop_distr}
  q(\bp_i;\bp_j) = \prod^{N_p}_{k} \frac{1}{\sqrt{2 \pi \sigma^2_k}} 
                   \exp \left( \frac{(p_{j,k} - p_{i,k})^2}{2 \sigma^2_k} \right).
\end{equation}
Since $q(\bp_{i-1};\bp') \equiv q(\bp';\bp_{i-1})$, the acceptance probability is simplified to
\begin{equation}
  \label{accept_probab2}
  \alpha =  \min \left( \frac{P(\bx|\bp')P(\bp')} {P(\bx|\bp_{i-1})P(\bp_{i-1})}, 1 \right).
\end{equation}
Obviously, the numerator and denominator in~\eqref{accept_probab2} are the right hand sides of~\eqref{bayes_propor} for the new and previous $\bp$, respectively, which in turn are proportional to the desired probability distribution. If the probability of proposal $\bp^\prime$ set is greater, then $\alpha = 1$, and $\bp^\prime$ becomes the new parameter set unconditionally. Due to the Gaussian likelihood, i.e. uncertainty in the observations~\eqref{gaussian_likelihood}, the acceptance probability $\alpha$ becomes
\begin{eqnarray}
  \label{accept_probab3}
  \alpha =  \min \biggl( \exp \left\lbrace -\frac{1}{2}\left( \chi^2(\bx;\bp') - 
                        \chi^2(\bx;\bp_i) \right)  \right\rbrace     \times              \nonumber \\
            \frac{P(\bp')} {P(\bp_i)}, 1 \biggr) .    
\end{eqnarray}
If the proposed parameter set were accepted only in case $\alpha=1$ when the new point is necessarily better (with lower $\chi^2$) than the previous one, the algorithm would be the basic random Monte-Carlo search. Unfortunately, the basic random search suffers from the ``curse of dimensionality": the rejection probability exponentially grows with the number of dimensions. Hence, a basic random search of many parameters will last forever. \citet{Metropolis_etal_1953} suggested a way out: accept not only $\chi^2$-better parameter sets, but also the sets that worsen $\chi^2$, but accept it with the probability $\alpha$. This technique ensures the ``random walk" of $\bp_i$, exploring the parameter space and visiting the volumes with better posterior probability more frequently than others. If the proposal set is rejected, the previous state will be repeated in the chain.

For models with many parameters the acceptance probability $\alpha$ tends to become small if all the parameters are stepped simultaneously, lowering the rate of acceptance and the overall algorithm efficiency. For this reason at each iteration we step only one parameter, keeping others constant. The following pseudocode describes one MCMC algorithm iteration:
\begin{enumerate}
\item Randomly choose $p_{i-1,j}$ from $\bp_{i-1}$, parameter number $j$ uniformly distributed;
\item Generate the $j$-th proposal parameter $p_{i-1,j}$ from the Gaussian distribution;
\item Calculate $\alpha$ and accept or reject $p_{i-1,j}$ with probability $\alpha$;
\item Repeat 1-3 for $N_p$ times; Memorize the newly generated state as $\bp_i$.
\end{enumerate}

The efficiency of this algorithm is also sensitive to the step size of proposal distribution~\eqref{gaussian_prop_distr}, which is determined by the variance of the Gaussian distribution. If it is too small, most of the trial points are accepted, but the random walk is too slow to sample all the parameter space. Conversely, if the step is too large, most of the trial points are rejected and the MCMC algorithm can get stuck at a certain point for a long time despite the ability to make large jumps. Previous empirical studies recommend optimizing the step size to make the accept rate $\sim 25$\% in high-dimensional cases \citep[see references in][]{Gregory2005}. The second, burn-in stage of MCMC is intended to adaptively adjust steps for all the parameters. After updating a $j^{th}$ parameter $p_{i,j}$, if the accept rate of newest 100 trials is more than 30\%, then the variance $\sigma_j$ is multiplied by 1.01. Otherwise, if the accept rate of newest 100 trials is less than 20\%, the variance $\sigma_j$ is divided by 1.01. 



\subsection{Replica Exchange MCMC Algorithm}

The described Metropolis-Hastings MCMC algorithm is quite suitable for our problems where the direct sampling is complicated or impossible. However, the original Metropolis-Hastings MCMC algorithm can fail to fully explore the target probability distribution, especially if the distribution is multi-modal with widely separated peaks. The algorithm can get trapped in a local mode and miss other regions of parameter space that contain significant probability. 

The replica-exchange MCMC algorithm (also known as parallel tempering) is a result of improvement of the MCMC algorithm targeted to such complex multi-modal distributions. The replica-exchange algorithm belongs to the class of ``generalized-ensemble algorithms". It has been developed mostly in the past decade and recently was applied to some astronomical problems \citep{Gregory2005,Varghese_etal2011,Benneke_Seager2012}. In this method a parameter $\beta$ called ``temperature" is introduced as
\begin{eqnarray}
  \label{post_distr_beta}
  P(\bp|\bx;\beta) &=&  \frac{P(\bx|\bp)^\beta P(\bp)}{\int_{\bp} \int_{\beta} 
                              P(\bx|\bp)^\beta P(\bp) \ud \bp \ud \beta} \nonumber \\
      &\propto& P(\bx|\bp)^\beta P(\bp)  .
\end{eqnarray}
When $\beta=1$, it becomes the target posterior distribution. For the Gaussian likelihood \eqref{gaussian_likelihood} the latter can be rendered as
\begin{equation}
      P(\bp|\bx;\beta) \propto \exp \left( \beta L(\bx|\bp) \right) P(\bp),
\end{equation}
where $L(\bx|\bp)$ is a log-likelihood. The term ``temperature" is borrowed from the canonical distribution $\exp \left( -\beta E \right)$ in statistical mechanics, where the absolute temperature is expressed using the ``thermodynamic $\beta$" written as 
\begin{equation}
	\beta = \frac{1}{kT},
\end{equation}
so $\beta$ is inversely proportional to the temperature. Using this analogy one can see that in Eq.~\eqref{post_distr_beta}  the log-likelihood $L(\bx|\bp)$ plays the role of negative energy $-E$. Higher temperatures (those with smaller $\beta$) make the likelihood function flatter and also make the Metropolis-Hastings acceptance probability $\alpha$ higher, because
\begin{eqnarray}
  \label{accept_probab_beta}
  \alpha = \min \biggl( \exp \left\lbrace -\frac{\beta}{2}\left( \chi^2(\bx;\bp') - \chi^2(\bx;\bp_i) \right)
              \right\rbrace \nonumber \\
         \times  \frac{P(\bp')} {P(\bp_i)}, 1 \biggr).  
\end{eqnarray}
Thus, the Metropolis-Hastings sampling at higher temperatures enables exploration of wider ranges of the parameter space. 

In the replica exchange MCMC algorithm, multiple Markov chains with different temperatures $(\beta_1,\beta_2, ... \beta_{N_\beta})$ including a chain with the lowest temperature $\beta=1$ and different initial conditions are generated in parallel. The specific values of $\beta_l$ usually span several orders of magnitude with logarithmic steps. As an example, 40 Markov chains may have $10^{-4} \leqslant \beta_l \leqslant 1$ with $l \in \overline{1\,..\,40}$. At each MCMC iteration, when the generation of new sets of parameters $\bp_{i,l}$ is finished in all the chains, the newly generated elements of adjacent chains at the temperatures $\beta_l$ and $\beta_{l+1}$ are exchanged with a probability $\alpha$ written as
\begin{equation}
  \label{exchange_probab}
  \alpha =  \min \left( \frac{P(\bp_{i,l+1}|\bx; \beta_{l+1})} 
            {P(\bp_{i,l}|\bx; \beta_{l})}, 1 \right) .
\end{equation}
The exchange procedure is repeated for $N_\beta - 1$ times, after which a new parameter $\bp_{i+1}$ generation begins. Under the Gaussian  likelihood \eqref{gaussian_likelihood} and the Gaussian proposal distribution \eqref{gaussian_prop_distr} it becomes
\begin{eqnarray}
  \label{accept_probab_beta2}
  \alpha &=& \min \biggl( \exp \biggl\lbrace -\frac{1}{2}(\beta_{l+1} - \beta_{l}) \nonumber \\
         &&\times  \Bigl( \chi^2(\bx;\bp_{i,l}) -
                 \chi^2(\bx;\bp_{i,l+1}) \Bigr) \biggr\rbrace  \nonumber \\
         &&\times \frac{P(\bp')}{P(\bp_i)}, 1 \biggr)      .
\end{eqnarray}
In the higher temperature distributions $(\beta \ll 1)$, radically new configurations are explored, while lower temperature distributions  $(\beta \approx 1)$ allow for detailed exploration of new configurations and local modes. The final inference on the model parameters is based on samples drawn from the target probability distribution $(\beta=1)$ only.

%Finally we would like to note that the described MCMC method is not limited to the search for a model parameters: the set (or tuple) $\bp$ may be any set of unknowns in wide range of problems.

\begin{figure}[ht]    %[b]
\plotone{mcmc_cuda_hardware.eps}
\caption{\small Two-level NVIDIA GPU architecture in terms of CUDA platform, GPU hardware, and MCMC-RE implementation.
\label{mcmc_cuda_hw}}
\end{figure}


\section{MCMC-RE GPU Implementation Details}

A single Markov chain itself cannot be computed in parallel, for each new ``chain link"
requires computing the previous one. However, in the Replica Exchange
version of MCMC several tens of Markov chains for different temperatures
are to be computed, and this can be done in the hardware parallel threads
on a GPU. Moreover, to multiply chances of the algorithm convergence
to the solution in shorter time, several such multi-temperature multi-thread 
processes can be run in parallel.

In this section we describe the implementation internals most of which are hidden 
from the user ``under the hood". However, they can help one understand how to 
use the upper-level, user-friendly software.


\subsection{Mapping MCMC-RE Algorithm on CUDA Architecture}

The CUDA software platform provides a two-level thread organization: a grid of 
blocks, where each block contains multiple threads. It is shown in Fig.~\ref{mcmc_cuda_hw}.
In terms of the NVIDIA GPU hardware, each block of threads is executed on
one of the Streaming Multiprocessors (SM). However, the architecture is
totally scalable, each SM can execute multiple blocks. The numbers differ
from model to model. For example, the GPU of GeForce GTX 670 videocard
has 7 SMs, with 192 processor cores per an SM. Another videocard,
GeForce GTX 1050 Ti, has 6 SMs, each containing 128 processor cores. 

The massively parallel MCMC-RE implementation fits in naturally with the
GPU architecture. The group of Markov chains running in parallel for all the set 
of temperatures make up a block of threads. We call such group a \emph{sequence}.
In terms of MCMC-RE the number of treads in one block is the number of 
temperatures $N_\beta$, and the number of blocks is the number of independent
sequences $N_{\rm seq}$. The user can set these variables to any numbers.
We prefer to use $N_\beta = 32$ because the unit of parallel execution, a
\emph{warp}, consists of 32 threads. Several warps can be simultaneously 
run on the same Streaming Multiprocessor (SM), so multiples of 32 are
preferred for $N_\beta$. However, we do not see any advantages in having
more than 32 temperatures.  The value of $N_{\rm seq}$ may be arbitrary.
It can be found experimentally as that providing the fastest computation. 
Intuitively clear, though, that $N_{\rm seq}$ should not be less than the
number of hardware SMs, and it would be reasonable to set it to a multiple
of the SM numver. In the subsections describing the arrays
used by the MCMC-RE code one can notice that many of them have 
two additional dimensions \verb|[nbeta,nseq]|, which corresponds to
$N_\beta$ and $N_{\rm seq}$.

The CUDA C language built-in variables allow a thread to always
know in which block it is running and what is its own thread number 
within the block: \verb|blockIdx| and \verb|threadIdx|. Two more variables,
\verb|blockDim| and \verb|threadDim|, contain the values of total number
of blocks and total number of threads in a block, respectively. Therefore,
in the MCMC-RE CUDA code the following, where possible, is observed: \\
\verb|blockDim| $ = N_{\rm seq}$, \verb|threadDim| $ = N_\beta$, \\
\verb|blockIdx| $ = i_{\rm seq}$, the sequence index, and \\
\verb|threadIdx| $ = i_\beta$, the temperature index. 

The MCMC algorithm was initially used for fitting parameterized models
to large arrays of data located at many coordinate points, like model 
of a celestial radio object observed with a radio interferometer. 
The data there are the visibilities, and the coordinates are
$u$ and $v$, the locations of visibilities in the spatial frequency domain.
This is why the input arrays are called \verb|dat| and \verb|coor|.
The data are provided to the algorithm in the \verb|dat[ndat]| array, and
the coordinates -- in the \verb|coor[ncoor]| array.  
Sometimes the model requires
integer data, passed in the integer array \verb|idat[nidat]|, and integer
coordinates (like antenna numbers) passed in the \verb|icoor[nicoor]| array.
However,  in other applications the arrays \verb|dat, coor, idat, icoor| 
can be used arbitrarily. Note that the whole of \verb|dat[ndat]| content, 
and only it, is used to compute $\chi^2$. On the same reason all the functions
to be optimized are located in the file \verb|model.cuh|. It contains the 
only C/C++ function \verb|model()|. In order to create an application to
solve a new problem, the user must write its own \verb|model()| function.

The \verb|model()| function works in a single thread, calculates only one 
number and stores it in a specified location of the \verb|datm[ndatm]| array. 
When writing the code of \verb|model()| function it is important to understand
the general picture of what is going on. We are looking 
for the best, optimal set of some function parameters, which secure its (global)
minimum. Let them be, for instance, tuple of the \verb|xringaus| model parameters
$(Z_{\rm sp}, \rex, \rin, \ d, \ f, \ a, \ b, \ g_q, \ \theta)$.   
At the beginning of each iteration we have $N_\beta \times N_{\rm seq}$ such 
parameter sets. The \verb|gen_proposal| function randomly selects in each tuple
one parameter and adds to it a random step, all in parallel. Eventually, all 
the $N_\beta \times N_{\rm seq}$ parameter tuples (or sets) are ``stepped", and 
the optimized function (or model) needs to be called many times to create the
``model" data sets, each with the same number of elements, $N_{\rm dat}$, as 
in the \verb|dat[ ]| array. This model calculation occurs totally in parallel:
the \verb|model()| function is called once for each one datum in \verb|datm[ndatm]|,
or in $N_\beta \times N_{\rm seq} \times N_{\rm dat}$ threads. This number of
threads is $N_{\rm dat}$ times more than $N_\beta \times N_{\rm seq} $ used by 
the MCMC algorithm. Therefore, the kernel \verb|calc_chi2_terms()| that calls 
\verb|model()| and, in each thread, calculates a single term of $\chi^2$ sums,
is invoked in as many of 64 thread blocks as needed. At each thread
\verb|model()| is provided with the index into \verb|pcur[ ]| to get the correct
parameter set, the index into \verb|dat[ ]|, and the index into \verb|datm[ ]|, 
where the computed value should be saved.

After that all the $N_\beta \times N_{\rm seq}$ ``model" data sets are
compared with the only data set in \verb|dat[ ]| by computing 
$N_\beta \times N_{\rm seq}$ values of $\chi^2$, and some of the
new parameter tuples are accepted and some rejected, according to the 
Metropolis algorithm in the \verb|spmp_mcmc()| function. 

To compute $\chi^2$ the algorithm requires the variance ${\sigma^2}$.
The \verb|std2r[ndat]| array should contain the reciprocals $1/\sigma^2$ 
of the data variances. This format is chosen out of the speed considerations,
since the massive divisions by $\sigma^2$ in the $\chi^2$ calculations are 
several times slower than the multiplications by $1/\sigma^2$. 

The MCMC with replica exchange is implemented as a function \verb|mcmc_cuda()|
(in the file \verb|mcmcjob.cu|) called by the Cython-compiled interface extension 
module \verb|mcmc_interf.so| (the source Cython script is in \verb|mcmc_interf.pyx|).
In turn, \verb|mcmc_cuda()| creates \verb|mcmcobj| as an instance of the class 
\verb|CMcmcFit| (defined in \verb|mcmcjob.h|). This object at the time of its creation
transfers the arrays to GPU and calls the class method \verb|do_mcmc_on_gpu()|
(also in \verb|mcmcjob.cu|). It, in turn, calls the \verb|run_mcmc()|, a CUDA C 
function determined with the rest of CUDA MCMC code in \verb|gpu_mcmc.cu|. 

In the Python codes the types of the parameters must strictly correspond to 
their  C and C++ analogs. Note that the implementation of CUDA MCMC-RE described
here uses the single precision 32-bit floats and 32-bit integers, \verb|float| 
and \verb|int|. Their Python analogs are \verb|np.float32| and \verb|np.int32|, 
where \verb|np| is the standard \verb|numpy| module. For the optimization problems 
the single precision appeared to be sufficient. Also, the GPU works faster with the 
single precision numbers, and they take less of the GPU memory.


\begin{figure}[ht!]    %[b]
\plotone{gpu_mcmc_software_hierarchy.eps}
\caption{\small Parallel MCMC-RE software hierarchy.
\label{mcmc_soft_hierarchy}}
\end{figure}



\subsection{Parallel MCMC-RE software hierarchy}

The MCMC software has been written in C/C++ and Python and includes several levels. Their hierarchy is depicted in Fig.~\ref{mcmc_soft_hierarchy}. The top level Python module \verb|imgpu| (file \verb|imgpu.py|) contains two classes, a universal solver \verb|mcgpu| and its problem-specific heir, or subclass \verb|mcgpu_sgra(mcgpu)|, which is intended for solving a problem of fitting the Sgr A* black hole model to the observation data. 

When a user creates an instance of either class, its constructor automatically creates and initializes all the variables and arrays needed for the MCMC execution. To start it, the user only needs to invoke the class method \verb|burnin_and_search()|. The two other methods, \verb|calcmodel| and \verb|calcmodchi2()|, allow fast computation of the model outputs (in case of \verb|mcgpu_sgra| -- visibilities, closure phases) and $\chi^2$ values for very large numbers of parameter sets in parallel on the GPU. The \verb|imgpu| module provides maximum convenience and makes application programs way shorter.

The next level down in the hierarchy is presented by the \verb|mcmc_interf| module (file  \verb|mcmc_interf.so|) built from the Cython source code \verb|mcmc_interf.pyx|. It is a Python-C/C++ interface module whose functions, \verb|mcmcuda()|,   \verb|calcmodel()|,   \verb|calcmodchi2()|, and  \verb|reset_gpu()| call underlying C/C++ functions. They provide convenience of programming in Python. However, in order to directly use the functions from \verb|mcmc_interf| the user in the Python application has to correctly initialize all the variables and arrays.  

The CUDA C/C++ file \verb|mcmcjob.cu| provides functions and classes forming the underlying level used by \verb|mcmc_interf|. The user writing the application codes in C/C++ can directly use the functions from \verb|mcmcjob.cu| or classes from \verb|mcmcjob.h|.    

According to CUDA terminology, we call the CPU a ``host", and the physically separate GPU unit,
working as a coprocessor, a ``device". We use NVIDIA graphic cards installed in
the PCI-e slots. The host and the device possess their own separate memory spaces, the host 
memory and the device (GPU) memory. Allocation of the device memory and moving the data to and from the device is controlled by the CPU C code through the CUDA runtime API functions.  

Preparing the data arrays, the device memory allocation, moving the data to the GPU device memory before 
the MCMC run and moving the generated data with the solution(s) back to the host memory after the MCMC run
is performed by the \verb|mcmc_cuda()| function in \verb|mcmcjob.cu|. It uses the class \verb|CMcmcFit| derived from the class \verb|CCalcModel|. Declaring \verb|mcmcobj| as a class \verb|CMcmcFit| object triggers the class constructor that allocates device memory, moves the data to the device, and launches the \verb|do_mcmc_on_gpu()| method. The latter, in turn, calls \verb|run_mcmc()| in \verb|gpu_mcmc.cu|. Tho other functions, \verb|calc_model()| and \verb|calc_modchi2|, require only a small subset of the MCMC data. They create the \verb|modobj| instance of the base class \verb|CCalcModel|, whose constructor and destructor allocate/move much less data. 

The last, lowest level of the hierarchy is located in the CUDA C/C++ file \verb|gpu_mcmc.cu|. In order to launch the massively parallel MCMC processes in the GPU memory, the class method \verb|CMcmcFit::do_mcmc_on_gpu()| calls the C program \verb|run_mcmc()| from \verb|gpu_mcmc.cu|. The GPU parallel processes are called \emph{kernels} and have the attribute \verb|__global__|. The MCMC implementation described here consists of several kernels, and they are called from the host code one after another, following the concept of "heterogeneous programming". 

The MCMC ``driver" program \verb|run_mcmc()| on entry gets two arguments, both are the pointers to the \verb|CMcmcFit| class objects. The first one, \verb|mc_h|, points to the host memory and the object it points at has the host pointers to the host data arrays. The second argument, \verb|mc_d|, is a pointer into the GPU device memory, and its class member pointers are into the device memory. This device pointer is further used as a parameter for the kernels: due to it the kernels running on the GPU have full access to all the data in the device global memory.


\subsection{Parameters of the MCMC-RE Algorithm} \label{algparams}

\subsubsection{Input Parameters}


Below is the list of parameters accepted by the \verb|mcmc_cuda()| function. 
It resides in the \verb|mcmcjob.cu| file and can be called directly from a C/C++ code. 
From Python it can be called from the module \verb|mcmc_interf| as \\
\verb|mcmc_interf.mcmcuda()| with the same list of parameters.

\verb|nptot|: total number of the parameters to be optimized by MCMC, or
       the full problem dimensionality. Part of the parameters in the
       \verb|ptotal[nptot]| array can be made immutable for a particular MCMC run
       by setting zeros at the corresponding locations in the descriptor
       array \verb|pdescr[nptot]|.

\verb|nprm|: the number of parameters to be optimized by MCMC in a specific run.
      It is possible to "freeze" values of a part of the problem parameters
      making them immutable constants, and optimize only the rest of them.
      The frozen (immutable) parameters must be marked by zeros at their
      positions in the descriptor array \verb|pdescr[nptot]|. Thus \verb|nprm| is the
      number of non-zero elements in \verb|pdescr[ ]|.

\verb|ndat|: number of floating point data.

\verb|ncoor|: number of floating point coordinates.

\verb|nidat|: number of integer data.

\verb|nicoor|: number of integer coordinates.

\verb|nseq| $\equiv N_{\rm seq}$: number of independent parallel processes of model fitting. In CUDA
      framework it means the number of thread blocks. In the Nvidia
      GPU hardware one block of threads is executed on one
      "Streaming Multiprocessor" (SM). For example, GTX 670 has 7 SMs. 

\verb|nbeta| $\equiv N_\beta$: number of ``temperatures" in the MCMC algorithm. In CUDA framework
       it means the number of parallel theads per block. In Nvidia GPU
       the treads in a SM are executed in units of ``warps" of 32 threads
       each executing the same instruction at a time.
       
\verb|beta1|, \verb|betan|: parameters of the Python classes \verb|imgpu| and \verb|imgpu_sgra|. 
	        They set the values in the array of temperatures \verb|beta[nbeta]| from \verb|beta[0] = beta1| 
            to \verb|beta[nbeta-1] = betan|. The temperature corresponds to $\beta = 1/{kT}$.
	        The lowest temperature, \verb|beta[0]|, is usually set to \verb|beta1 = 1.|, 
	        the highest, \verb|beta[nbeta-1]|, can be set to several orders of 
	        magnitude less, something like \verb|betan = 0.0001|
	        The values in \verb|beta[nbeta]| fall off exponentially, i.e. as a geometric progression 
	        with common ratio \verb|(betan/beta1)|$^\frac{1}{\mathrm{nbeta-1}}$, from 1 down to \verb|betan|.

\verb|seed|: an arbitrary 64-bit unsigned integer (np.uint64) seed for the CUDA
      random number generator. 

\verb|pdescr[nptot]|: array of parameter descriptors with possible values
               2 - angular parameter (radians), 1 - nonangular parameter.
               0 - this value excludes the corresponding parameter in
                   \verb|ptotal[nptot]| from optimization.
                   
\verb|ptotal[nptot]|: array of model parameters. The \verb|ptotal[ ]| values at
               the locations where \verb|pdescr[ ]| is nonzero are ignored.
               The values where \verb|pdescr[ ]| is zero are used in the model.
               
\verb|pmint[nptot]|,

\verb|pmaxt[nptot]|: minimum and maximum values for the model parameters.
              The parameters are searched only inside of the
              nptot-dimensional rectangular parallelepiped determined by
              \verb|pmint[ ]| and \verb|pmaxt[ ]|. This parallelepiped determines the
             ``prior".

\verb|ivar[nprm]|: maps [nprm] optimized parameters on ptotal[nptot].
            ivar[nprm] has indices of the ptotal[nptot] parameters whose
            descriptors in pdescr[nptot] are non-zero.

\verb|invar[nptot]|: maps ptotal[nptot] on the varied parameters [nprm]. If some
              of the parameters are immutablem invar[ ] must contain -1 values
              at the corresponding positions.

\verb|beta[nbeta]|: ``temperature" values in descending order from 1. Recommended are
             the values between 1 and 0.0001 falling off exponentially, i.e. as a geometric progression
	         with common ratio $0.0001^\frac{1}{\mathrm{nbeta-1}}$, from 1 down to 0.0001.


\verb|dat[ndat]|: input floating point data (e.g. visibilities, phases etc).

\verb|idat[nidat]|: input integer data.

\verb|coor[ncoor]|: input floating point coordinates (e.g. u and v pairs).

\verb|icoor[nicoor]|: integer coordinates (e.g. antenna numbers)
             
\verb|std2r[ndat]|: the reciprocals of the data variances, $1/\sigma^2_i$.

\verb|pcur[nbeta,nseq,nptot]|: initial parameter values for all temperatures and
                        all sequences. Can be zeros or random numbers or
                        the coordinates of the prior center. During MCMC
                        run is used for the current parameter values.

\verb|tcur[nbeta,nseq]|: integer values of the ``temperature" indices. On entry it
                  should contain nseq columns of cardinals from 0 up to
                  \verb|nbeta-1|.

\verb|n_cnt[nprm,nbeta,nseq]|: numbers of Metropolis trials for each mutable
                        parameter, each temperature, and each sequence.
                        Should be initialized to ones.

\verb|nadj|: number of sets for calculating acceptance rates. Recommended value 100.

\verb|npass|: number of times the \verb|model()| function must be called. Sometimes
       the model computation requires several passes. The data generated and saved in a
       previous pass are used in the next one. The \verb|model()| function
       can determine at which pass it has been called by its parameter \verb|ipass|
       taking values from 0 to \verb|npass-1|.

\verb|pstp[nprm,nbeta,nseq]|: initial parameter steps for each mutable parameter,
                       each temperature, and each sequence.
                       
\verb|imodel|: an integer parameter passed to the \verb|model()| function. The usage is
        arbitrary. For example, it may be a model number to select between
        several models.

\verb|nburn|: number of the burn-in iterations. During the burn-in phase the steps
       for each parameter, each temperature, and each sequence are adjusted,
       and the transients fall off.

\verb|niter|: number of optimization iterations.

\verb|ndatm|: must be equal \verb|nbeta*nseq*ndat|.

%=============================================================================


\subsubsection{Workspace Arrays}

\verb|datm[nbeta,nseq,ndat]|: computed model data.

\verb|chi2m[nbeta,nseq,ndat]|: $\chi^2$ terms for each model data element, each
                        temperature, and each sequence.

\verb|rndst[nbeta,nseq,48]|: unsigned 8-bit integer \verb|(np.uint8)| array of states
                      for the random number generator.

\verb|flag[nbeta,nseq]|: 0 means the selected parameter is outside of the prior,
                  or the parameter did not pass the alpha-test.
                  1 means the selected parameter is OK.
                  
\verb|chi2c[nbeta,nseq]|: The $\chi^2$ values for each temperature and each sequence 
                          memorized from the previous Metropolis step to be compared
                          with the new ones.

\verb|ptent[nbeta,nseq]|: tentative parameter values for each temperature and each
                   sequence.

\verb|ptentn[nbeta,nseq]|: tentative parameter indices for each temperature and each
                    sequence.

%=============================================================================



\subsubsection{Output Parameters}

\verb|pout[nprm,nbeta,nseq,niter]|: mutable parameter values found by the MCMC for
                             all the temperatures, sequences, and iterations.
                             As a rule, only the coldest temperature is used,
                             i.e. \\
                             \verb|pout[nprm,0,nseq,niter]|. \\
                             The optimum parameter set is in \verb|pout[ ]| at the
                             location corresponding to the minimum $\chi^2$
                             in the \verb|chi2[nseq,niter]| array. Usually these
                             arrays reshape to 1D form:\\ 
                             \verb|po = pout[:,0,:,:].reshape((nprm,nseq*niter))|
                             \verb|c2 = chi2.flatten()| \\
                             and then the best parameter set is found as
                             \verb|po[c2.argmin()]|. \\
                             Also, the histograms of each
                             parameter can provide important information: \\
                             \verb|hist(po[0,:], 50, color='b')|
                             \verb|grid(1)| - for the 0-th parameter and so on.

\verb|chi2[nseq,niter]|: $\chi^2$ values for the highest temperature ($\beta=1$), 
                         for all the sequences and iterations.

\verb|n_acpt[nprm,nbeta,nseq]|: counting numbers of accepted proposal sets in the
                         Metropolis-Hastings algorithm.

\verb|n_exch[nbeta,nseq]|: counting numbers of exchanged adjacent temperature
                    chanins in the replica exchange algorithm.

\verb|n_hist[nadj,nprm,nbeta,nseq]|: counting numbers of ``accept" of proposal set
                              in Metropolis-Hastings algorithm




\subsection{Parallel MCMC-RE Algorithm in CUDA C/C++}

At each call, a kernel needs two special parameters enclosed in the \verb|<<<>>>| brackets:
the number of threads in the block, and the number of blocks in the grid. We run \verb|nseq|
independent MCMC processes (``sequences"), one per a block, and each process runs \verb|nbeta|
parallel threads, one per a temperature. Below the MCMC algorithm is described as a series of
kernel calls with comments. These codes are in the file \verb|gpu_mcmc.cu|, function \verb|run_mcmc()|. \\

1. Initialize the random number generators for all the $N_{\rm seq} \times N_{\rm beta}$ threads: \\ 
\verb|  init_rng<<<nseq,nbeta>>>(mc_d);| \\

2. Initialize the optimized parameters for every sequence and every temperature by assigning to them the random values uniformly distributed inside of the prior. They become the current parameters: \\
\verb|  init_pcur<<<nseq,nbeta>>>(mc_d);| \\

3. Compute all the $\chi^2$ terms like \\
$({\rm datm}[i_{\rm beta},i_{\rm seq},i_{\rm dat}] - {\rm dat}[i_{\rm dat}])^2/\sigma^2[i_{\rm dat}]$, \\
where \verb|dat[ndat]| are the observation data (visibility amplitudes and closure phases), and \\
\verb|datm[nbeta,nseq,ndat]| are the results of computing \verb|nbeta x nsec| models. The $\chi^2$ terms are 
saved in the array \verb|chi2m[nbeta,nseq,ndat]|. This computation is fulfilled in parallel on \verb|nblocks| with \verb|blocksize| threads per a block, arbitrarily set to 128. So, \\ 
\verb|nblocks = nbeta*nseq*ndat/blocksize + 1|. \\

The kernel is called in a loop \verb|npass| times with \verb|ipass| running from 0 to \verb|npass-1|: \\
\verb|  calc_chi2_terms<<<nblocks,| \\ 
\verb|              blocksize>>>(mc_d, ipass);| \\
    
4. From the terms in \verb|chi2m[nbeta,nseq,ndat]| compute  $\chi^2$ and store the values in the \\
\verb|chi2m[nbeta,nseq]| array: \\
\verb| calc_chi2<<<nseq,nbeta>>>(mc_d);| \\

After the initialization the two MCMC stages follow: burn-in and main. At the burn-in stage 
the step sizes in \verb|pstp[nprm,nbeta,nseq]|
for each parameter in each sequence, and each temperature are adjusted based on the parameter acceptance/rejection rate calculated from the history being accumulated in \\
\verb|n_hist[nadj,nprm,nbeta,nseq]|.
 The burn-in stage 
requires a few hundred iterations, which is usually less than that for the main stage. 
Both stages are very similar. Actually, the burn-in is a ``dry run" of the MCMC-RE, where
the steps are adjusted and 
the \verb|replica_exchange()| kernel does not save in the array \\
\verb|pout[nprm,nbeta,nseq,niter]| \\
the new parameter sets found. At the burn-in stage the parameter \verb|burn| of \\
\verb|replica_exchange(mc_d, burn, itr)| 
 must be non-zero. At the main stage \verb|burn| is set to zero. \\

4. Burn-in stage: \\\\
\verb|  burn = 1;| \\
\verb|  for (itr = 0; itr < nburn; itr++) {| \\
\verb|    for (ipm = 0; ipm < nprm; ipm++) {| \\

In \verb|gen_proposal()| to a randomly \\
chosen parameter a random "Gaussian" step with the standard deviation from 
\verb|pstp[nprm,nbeta,nseq]| is added. If the parameter gets outside of the prior, it is flagged by 0 in 
\verb|flag[nbeta,nseq]|. Otherwise its index is 
saved in
\verb|ptent[nbeta,nseq]| as ``tentative": \\ \\
\verb|      gen_proposal<<<nseq,nbeta>>>(mc_d);| \\

Compute $\chi^2$ terms \verb|npass| times for \verb|ipass| from 0 to \verb|npass| - 1: \\ \\
\verb|      calc_chi2_terms<<<nblocks,| \\
\verb|             blocksize>>>(mc_d, ipass);| \\

Single Parameter Metropolis MCMC. The proposed tentative parameter is accepted with a probability $\alpha$ calculated by the formula in Eq.~\eqref{accept_probab3}: \\ \\
\verb|      spmp_mcmc<<<nseq,nbeta>>>(mc_d);| \\ \\
\verb|      adjust_steps<<<nseq,nbeta>>>(mc_d);|  \\
\verb|    }| \\ \\
In \verb|replica_exchange()| two randomly selected adjacent temperature chains exchange their parameters and $\chi^2$ with the probability calculated as given in Eq.~\eqref{accept_probab_beta2}. This is repeated
$N_\beta-1$ times. Unfortunately, it cannot be parallelized into $N_\beta$ temperature threads, so $N_{\rm seq}$ independent MCMC processes execute this simultaneously in blocks with one thread per block: \\ \\ 
\verb|    replica_exchange<<<nseq,1| \\
\verb|                    >>>(mc_d, burn, itr);| \\
\verb|  }| \\ \\


5. Main stage. It is not commented because comments to the burn-in stage explain both stages. \\ \\
\verb|  burn = 0;| \\\\
\verb|  for (itr = 0; itr < niter; itr++) {| \\
\verb|    for (ipm = 0; ipm < nprm; ipm++) {| \\\\
\verb|      gen_proposal<<<nseq,nbeta>>>(mc_d);| \\\\
\verb|      for (ipass = 0; ipass < npass;| \\
\verb|                             ipass++) {| \\
\verb|        calc_chi2_terms<<<nblocks,| \\
\verb|              blocksize>>>(mc_d, ipass);| \\
\verb|      }| \\
\verb|      spmp_mcmc<<<nseq,nbeta>>>(mc_d);| \\
\verb|    }| \\
\verb|    replica_exchange<<<nseq,1| \\
\verb|                    >>>(mc_d, burn, itr);| \\
\verb|  }| \\

6. The results are moved to the host memory. The parameters of the model, the coordinates of the absolute minimum, or the roots found in the MCMC main stage run are in the array 
\verb|pout[nprm,nbeta,nseq,niter]|, and the $\chi^2$ values for the coldest temperature ($\beta=1$) are in the array \verb|chi2[nseq,niter]|. Using the latter, the best result in \verb|pout[ ]| can be found at the location of the $\chi^2$ minimum.


 
\section{Using the MCMC-RE Software}

In order to use the software for fast numerical solving a specific optimization
problem, the user should write two programs: a script in Python and a CUDA C/C++ 
function \verb|model()|. The Python script is intended for running the MCMC-RE 
software, which, in turn, calls \verb|model()| in a highly parallel mode to 
compute the function(s) to be optimized or the equations to be solved. 

\subsection{Installation}

The current version of the CUDA MCMC-RE software works in Linux only. Its 
installation does not require the root privileges. The software is in 
\verb|cuda_mcmc.tar.gz| archive. Unpack it with the command \\\\
\verb|$ tar xvfz cuda_mcmc.tar.gz| \\\\
This creates the directory \verb"cuda_mcmc" with the following structure: \\

\verb"cuda_mcmc" \\
\verb"   |-- doc" \\
\verb"   |-- src" \\
\verb"   +-- template" \\
\verb"   +-- examples" \\
\verb"        |-- search_Rosenbrock" \\
\verb"        |-- sgra_model_fit" \\
\verb"        +-- solvsys" \\

Enter the directory and run \verb|make| command: \\\\
\verb|$ cd cuda_mcmc| \\
\verb|$ make| \\\\
The command creates two directories, if they do not already exist in the user's 
home directory: \verb|~/lib64/python| and \verb|~/bin|. The former will be used 
to install the Python modules, while the latter -- to install binary codes. 
These directories must be appended to the \verb|PYTHONPATH| and \verb|PATH| 
environment variables. One more environment variable, \verb|CUDA_MCMC|, must
point at the directory \verb|cuda_mcmc| from which \verb|make| has been invoked. 
The following three lines are added to the end of user's \verb| ~/.bashrc| file: \\\\
\verb|export PATH=$PATH:~/bin| \\
\verb|export PYTHONPATH=$PYTHONPATH:~/lib64/python| \\
\verb|export CUDA_MCMC=$(PWD)| \\\\

When \verb|make| exits, it asks the user to run the command \\\\
\verb|$ source ~/.bashrc| \\\\


\subsection{Working on a Project}

Just create a directory for your project in any location of your file tree and
copy there the contents of \verb|$CUDA_MCMC/template|. Rename \verb|template.py|
into a desired name. The file contains the general scheme of using the MCMC-RE
software. The \verb|model.cuh| file should be edited to include the codes for
computing the user's function(s) to be optimized. When both Python script and 
\verb|model.cuh| are ready, just run \verb|make|. The CUDA software in
\verb|$CUDA_MCMC/src| will be compiled with \verb|model.cuh| included from \
your project directory and the Python extension module \verb|mcmc_interf.so|
will be built and copied to \verb|~/lib64/python|. Running the Python script
will start the MCMC-RE algorithm with your project's \verb|model.cuh|. 

\subsection{Writing the model() function to be optimized}

For each individual fitting, optimization, or equation solving problem the user needs to write the \verb|model()| function, compile it and link it with the rest of the code. This function computes the optimized expression(s) working in parallel in all the threads of all the blocks. This function should be determined in a CUDA header file \verb|model.cuh|.

The \verb|model()| function is only called from the GPU device, from the kernel \\
\verb|calc_chi2_terms()|, so it has the attribute \verb|__device__| (i.e. callable from device only). 
Initially this function was intended to compute the model visibilities and closure phases of the supermassive  black hole at our Galaxy center observed with the VLBI array, hence the function name. However, it can be written to compute the left hand side of a vector or scalar expression like ${\bf F}({\bf x}, {\bf v}) = 0$.
The MCMC is able to search for the absolute minimum of the expression, or search for its multiple minima in the space determined by the parameter vector ${\bf x}$, while ${\bf v}$ are some arbitrary parameters set by the user. If ${\bf F}({\bf x}, {\bf v}) = 0$ is a consistent system of equations, MCMC is able to find all of its roots.

Each \verb|model()| call occurs in a single thread and computes only one data element. Here are the function parameters: \\ \\
\verb|iret = model(mc, idat, ipt, idatm, ipass);| \\

\verb|CCalcModel *mc|: pointer to the \verb|mc| object whose data structure contains all the working data for all the blocks and all the threads.

\verb|int id|: index into \verb|mc->dat|.

\verb|int ipt|: index of the first parameter in a set in \verb|pcur[nbeta,nseq,nptot]| for the specific temperature and specific sequence, determined by the caller function from the thread and block indices. So, \verb|mc->pcur[ipt]| can be treated as \verb|pcur[ibeta,iseq,:]| in Python code.

\verb|imd|: a ``through index" into \verb|datm[ibeta,isec,id]|, the model data.  

\verb|ipass|: pass number. The function can be called several times. For example, at pass 0 the visibility amplitudes and phases are calculated. At pass 1 the closure phases from already available phases are calculated. 

The result should be stored in \verb|mc->datm[imd]| location.

Also, \verb|iret = model()| returns are treated as follows. \\
\verb|iret == 0|: no results. \\
\verb|iret == 1|: success. \\
\verb|iret == 2|: the computed \verb|mc->datm[imd]| is an angle in radians. \\

Here is an example of \verb|model()| written for solving a quite elementary (but hard for the gradient methods) optimization problem: finding the minimum of the ``Rosenbrock's valley" or Rosenbrock's banana function 

$z = (1 - x)^2 + 100(y - x^2)^2$. \\

This is the code: \\ \\
\verb|__device__ int model(CCalcModel *mc, | \\
\verb|    int id, int ipt, int imd, int ipass) {| \\
\verb|  /* x: mc->pcur[ipt]; y: mc->pcur[ipt+1] */| \\
\verb|  if (id == 0)| \\
\verb|    mc->datm[imd] = pow(1.f - mc->pcur[ipt], 2)| \\
\verb|      + 100.f*pow(mc->pcur[ipt+1]| \\ 
\verb|            - pow(mc->pcur[ipt], 2), 2);| \\
\verb|  return 1;|\\
\verb|}|

Here we have only two parameters and only one data element, set to zero. Here we calculate $z$ in many sequences and many temperatures, but it is always compares with zero. The $\chi^2$ here is actually 
$z^2$, but it is what we need: minimize $z$ varying $x$ and $y$. 

\subsection{Writing Python Script: Class Mcgpu}

The user has two options, use the low-level API or the high-level class methods.
The API is implemented in the form of the \verb|mcmc_interf| module functions. 
This option requires preliminary initialization of a significant number of arrays
and parameters (see Subsection \ref{algparams} ``Parameters of the MCMC-RE Algorithm") and is
not recommended. The high-level class \verb|Mcgpu| from the module \verb|imgpu|,
on the contrary, is easy to use. When instantiated, its constructor creates all
the arrays and completes the necessary initializations ``under the hood". To
start the MCMC-RE optimization, the user simply calls the class method
\verb|burnin_and_search()| without parameters. Here we show the steps coded in
\verb|template.py|: \\\\
\verb|import imgpu| \\\\
\verb|# Create the 'solver' object of| \\
\verb|# the 'Mcgpu' class:| \\
\verb|solver = imgpu.Mcgpu(pdescr=pdescr1, \| \\
\verb|         pmint=pmint1, pmaxt=pmaxt1)| \\\\
\verb|# Start parallel MCMC-RE on the GPU:| \\
\verb|solver.burnin_and_search()| \\\\
The results are among the \verb|solver| object attributes. For example, the
optimal parameters found by the algorithm are in the array \verb|solver.pout|
at the location(s) determined by the minimum values in another array, 
\verb|solver.chi2|.  

All the instantiation parameters as well as the class attributes (or 
``class members" in C++ terminology) have \emph{exactly} the same names 
and meanings as those of the MCMC-RE parameters described in Subsection  \ref{algparams}. 
The only two exceptions are \verb|Mcgpu.pout_4d| and \verb|Mcgpu.chi2_2d|:
they correspond to \verb|pout| and \verb|chi2| of the algorithm. This is
because the method \\
\verb|Mcgpu.burnin_and_search()| transfers their contents
into more convenient 2-dimensional \\
\verb|Mcgpu.pout[nprm,nseq*niter]|
and 1-dimensional \verb|Mcgpu.chi2[nseq*niter]|. 

The class \verb|Mcgpu| instantiation only requires three mandatory parameters:

\verb|pdescr[nptot]|: The array of parameter descriptors. The model dimensionality
\verb|nptot| inside of \verb|Mcgpu| is assumed equal the number of \verb|pdescr|
elements. possible values are 2 - angular parameter (radians), 1 - nonangular parameter,
0 - this value excludes the corresponding parameter in \verb|ptotal| from optimization.

\verb|pmint[nptot]| and \verb|pmaxt[nptot]|: for any i-th of the \verb|nptot| dimensions
\verb|pmint[i]| and \verb|pmaxt[i]| determine the lower and upper limits between 
which the i-th parameter will be searched. 

Here is the full list of other parameters that can be used at its instantiation.

\verb|dat[ndat]|: The array of \verb|np.float32| data to compare with the model
output to \verb|datm[ ]| via computing $\chi^2$. In case of a 
function minimum search \verb|dat[ ]| can have any single value. If not specified,
1-element array with zero value is created by default.

\verb|coor|: Array of arbitrary size of the \verb|np.float32| data the user 
wants \verb|model()| to have access to. If not specified, 1-element array with 
zero value is created by default.   

\verb|std2r|, \verb|icoor|, \verb|idat|, \
				 \verb|ptotal|,  \
				 \verb|beta|, \verb|beta1|, \verb|betan|,	\
				 \verb|seed|, \verb|imodel=0|, \verb|npass=1|, \
				 \verb|nadj|=100, \verb|nbeta|=32, \verb|nseq|=14,  \verb|nburn|=300, \verb|niter|=500 


\section{Examples of Optimization and Solving Nonlinear Equation Systems}


\clearpage

\bibliographystyle{apalike}
\bibliography{blackhole_static_image_geom_reconstr}

\end{document}


